{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3000 ORD MAX \n",
    "# The whole process\n",
    "\n",
    "\n",
    "## Preprocess\n",
    "In the beginning we decided the features we wanted to use amd therefore we decided to remove some of the colums because they were not necesarry for our predictions. The more data the longer time it takes to process and the data may become more inaccurate. We decided to drop NaN values as well.\n",
    "\n",
    "### Why are we dropping \n",
    "\n",
    "It's worth noting that dropping rows with NaN values might lead to a loss of data. We have consider other methods, such as filling NaNs with mean, median, or a specific value, depending on the nature of the data. But we chose to remove them. \n",
    "\n",
    "The code drops columns using irrelevante_col to streamline the dataset for analysis. Columns are removed if they are irrelevant, have many missing values, contain redundant information, or exhibit inconsistencies. The decision is tailored to the analysis goals, aiming for a cleaner dataset that meets specific requirements and enhances overall data quality. As well as more uncessasary data can cause inaccurate results and a longer runtime. \n",
    "\n",
    "### Visulization of NaN values \n",
    "\n",
    "We are cleaning and preparing the data for it to be used. Here is a visulasation of a bar chart that visually represents the impact of dropping rows with missing values (NaN) in our DataFrame. \n",
    "We see that the count of missing values significantly decreases after dropping rows, it indicates that this operation is effective in reducing missing data. If there was not a masjor change it can be considered effective in maintaining data integrity. But as we can see it was amsrat move. \n",
    "Dropping rows can also be computationally efficient, especially our dataset is large and the proportion of rows with missing values is relatively small. So even of there was a lot of data that is not used now it is a small amount compared to the whole dataset. \n",
    "Of couse dropping values and its outcome depends on the dataset but considering this exaple we thought the smartest move was to remove to remove the values. \n",
    "\n",
    "The significant reduction in missing values post-removal indicates the effectiveness of this operation with minimal impact on data integrity.\n",
    "The computational efficiency of dropping rows is highlighted, especially for large datasets, where the loss is negligible compared to the dataset's overall size.\n",
    "\n",
    "### Fish types and why we chose the fish we did \n",
    "Concentrating on the \"Art FAO\" column, we excluded other species, prioritizing the analysis of predominant fish types for accuracy.\n",
    "Here you can see a visulasation of the different fish types. As you can see there is a significant difference between the fishes. To get the best accuracy and values we chose to focus on the fish with the most data. \n",
    "Making a barchart of all fish to visualize\n",
    "\n",
    "## KNN\n",
    "When we used KNN we decided to normolize, it can be smart to normalize the featurtes because then no feature is very different from one another \n",
    "We are leverling out the feild for all features so the KNN works better and make even more accurate predictions  \n",
    "\n",
    "#### Confusion matrix in KNN \n",
    "The diagonal elements of the confusion matrix represent the correct predictions, while the off-diagonal elements represent the incorrect predictions.   \n",
    "A good model is one which has high correct predictions while few false predictions . The Blue ones that goes diagnonal are the correct predictions. As you can see there are a significant higher number of true predicttions. We have decided to predict this amount of different fish because we wantted the predictions to be fairly correct. \n",
    "\n",
    "You can see the numbers within the 300-400 value range. We have concluded that these might be difficult to differenciate because these species are different types of cod. And therefore might be more difficult to differenciate. Some of the squares that are similar in numbers that are not diagnonal are similar because sei and hyse is a type of torsk. And therefore you can seee it is more difficult to predict.\n",
    "###### Taking a look at the data \n",
    "If we look at the data we can see that there is more Torsk (cod) than there is Sei. Considering we have more data of Torsk than of Sei we can see that the data is more accurate for the Torsk. This is because it has more data to compare and can become even more accurate. And continuing. \n",
    "\n",
    "## Supervised Logistic regression \n",
    "\n",
    "## Torch \n",
    "\n",
    "## DL\n",
    "In our DL model we have used to_categorical to convert categorical data (like class labels) into a numerical format that can be used as input to a neural network model. The function is necessary when working with classification tasks, as most machine learning algorithms require numerical inputs. It helps in improving the performance of the model by representing categorical data in a suitable format for computation. \n",
    "\n",
    "The Epochs represents the number of times the entire dataset has been passed forward and backward through the neural network during training.\n",
    "Loss is a measure of how well the model is performing.The loss decreases over epochs, indicating that the model is improving. Higher accuracy values indicate better performance. Similar to loss, both training accuracy and validation accuracy are reported during training. The accuracy increases over epochs, indicating that the model is learning and making more accurate predictions.\n",
    "The validation loss and accuracy metrics give insights into how well the model is generalizing to new, unseen data.\n",
    "\n",
    "After training is complete, the model is evaluated on a separate test dataset to assess its performance on completely unseen data. The reported test accuracy tells us how well the model is expected to perform in the real world.\n",
    "\n",
    "In summary, this output tells us how the model's loss and accuracy change over training epochs, as well as how well it performs on both the validation and test datasets. Overall, the decreasing loss and increasing accuracy suggest that the model is learning and improving its performance as it trains.\n",
    "\n",
    "## Clustering \n",
    "With using clustering we want to check if the different types of fish is living in different places using the cordinates, and if this is one of the huge impacts on the other ML models we are using\n",
    "\n",
    "## Comperison \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
